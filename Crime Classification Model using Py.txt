Crime Classification Model using Pyspark
We are interesting in a system that could classify crime discription into different categories. We want to create a system that could automatically assign a described crime to category which could help law enforcements to assign right officers to crime or could automatically assign officers to crime based on the classification.

We are using dataset from Kaggle on San Francisco Crime. Our responsibilty is to train a model based on 39 pre-defined categories, test the model accuracy and deploy it into production. Given a new crime description, the system should assign it to one of 39 categories.

To solve this problem, we will use a variety of feature extraction techniques along with different supervised machine learning algorithms in Pyspark.

This is multi-class text classification problem.


The process of cleaning the dataset involves:

Define tokenization function using RegexTokenizer: RegexTokenizer allows more advanced tokenization based on regular expression (regex) matching. By default, the parameter “pattern” (regex, default: “\s+”) is used as delimiters to split the input text. Alternatively, users can set parameter “gaps” to false indicating the regex “pattern” denotes “tokens” rather than splitting gaps, and find all matching occurrences as the tokenization result.

Define stop remover function using StopWordsRemover: StopWordsRemover takes as input a sequence of strings (e.g. the output of a Tokenizer) and drops all the stop words from the input sequences. The list of stopwords is specified by the stopWords parameter.

Define bag of words function for Descript variable using CountVectorizer: CountVectorizer can be used as an estimator to extract the vocabulary, and generates a CountVectorizerModel. The model produces sparse representations for the documents over the vocabulary, which can then be passed to other algorithms like LDA. During the fitting process, CountVectorizer will select the top vocabSize words ordered by term frequency across the corpus. An optional parameter minDF also affects the fitting process by specifying the minimum number (or fraction if < 1.0) of documents a term must appear in to be included in the vocabulary.

Define function to Encode the values of category variable using StringIndexer: StringIndexer encodes a string column of labels to a column of label indices. The indices are in (0, numLabels), ordered by label frequencies, so the most frequent label gets index 0. In our case, the label colum(Category) will be encoded to label indices, from 0 to 38; the most frequent label (LARCENY/THEFT) will be indexed as 0.

Build Multi-Classification
The stages involve to perform multi-classification include:

Model training and evaluation
Build baseling model
Logistic regression using CountVectorizer features
Build secondary models
Naive Bayes
Logistic regression and Naive Bayes using TF-IDF features
Logistic regression and Naive Bayes using word2Vec


The table below has accuracy of the models generated by different extraction techniques.

Logistic Regression	Naive Bayes
Count Vectoriser	97.2%	99.3%
TF-IDF	97.2%	99.5%
Word2Vec	90.7%	
Explanation: As you can see, TF-IDF proves to be best vectoriser for this dataset, while Naive Bayes proves to be better algorithm for text analysis than Logistic regression.







Developed a PySpark-based system for automated crime classification in San Francisco.
Trained models with 39 predefined categories, achieving high accuracy.
Dataset cleaning involved tokenization, stopword removal, and CountVectorizer.
Employed supervised ML algorithms, highlighting Naive Bayes's superior performance.